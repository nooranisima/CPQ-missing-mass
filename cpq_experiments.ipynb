{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f87f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/Server2019Test/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from scripts.calibration_module import *\n",
    "from scripts.sampling_module import *\n",
    "from scripts.utils import * \n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93eb519",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/Server2019Test/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Cell 1: imports & setup\n",
    "from scripts.utils import *\n",
    "from scripts.sampling_module import *\n",
    "from scripts.calibration_module import * (\n",
    "    compute_accuracy_and_calibration_threshold_optimal,\n",
    "    generate_prediction_sets_optimal\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# adjust to your filename in /data\n",
    "DATA_PATH = \"../data/GSMK8k_semantic_clusters.json\"\n",
    "dataset = load_dataset(DATA_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c124985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_accuracy(clusters, normalize_func):\n",
    "    \"\"\"\n",
    "    Computes best-of-N accuracy: fraction of instances where the true answer\n",
    "    appears among any of the sampled responses.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for inst in dataset:\n",
    "        # Run sampling & clustering; we only need clusters\n",
    "        responses, clusters, _, _, _ = simulate_and_cluster(\n",
    "            inst['prediction_history'],\n",
    "            inst['clusters'],\n",
    "            **algorithm_kwargs\n",
    "        )\n",
    "        # Normalize responses\n",
    "        resps_norm = [normalize_func(r) for r in responses]\n",
    "\n",
    "        # Normalize true answer\n",
    "        true_norm = normalize_func(inst.get('true_answer', \"\"))\n",
    "\n",
    "        total += 1\n",
    "        if true_norm in resps_norm:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc13577",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_algorithms_optimal(\n",
    "    full_results,\n",
    "    algorithms,\n",
    "    alpha: float = 0.2,\n",
    "    n_splits: int = 5,\n",
    "    test_size: float = 0.5,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare multiple sampling‐clustering algorithms over random splits.\n",
    "    Returns both mean and std of each metric across splits.\n",
    "\n",
    "    Returns:\n",
    "        metrics_summary: dict mapping alg_name -> dict of metrics:\n",
    "            {\n",
    "              'coverage': {'mean': ..., 'std': ...},\n",
    "              'ee_frac':  {...},\n",
    "              ...\n",
    "            }\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    N = len(full_results)\n",
    "\n",
    "    # Initialize accumulators\n",
    "    metrics_accum = {\n",
    "        alg['name']: {\n",
    "            'coverage': [], 'ee_frac': [], 'avg_size': [], 'accuracy': [], 'em_calib_accuracy': []\n",
    "        }\n",
    "        for alg in algorithms\n",
    "    }\n",
    "\n",
    "    indices = list(range(N))\n",
    "    for split in range(n_splits):\n",
    "        random.shuffle(indices)\n",
    "        cutoff = int(N * (1 - test_size))\n",
    "        calib_idx, val_idx = indices[:cutoff], indices[cutoff:]\n",
    "        calib_set = [full_results[i] for i in calib_idx]\n",
    "        val_set   = [full_results[i] for i in val_idx]\n",
    "\n",
    "        for alg in algorithms:\n",
    "            name = alg['name']\n",
    "            alg_kwargs = {k:v for k,v in alg.items() if k!='name'}\n",
    "            excluded = {'name', 'calibration_fn', 'prediction_fn'}\n",
    "\n",
    "            # --- Calibration Phase ---\n",
    "            calib_simulated = []\n",
    "            for inst in calib_set:\n",
    "                resp, clusters, sem_probs, history, num_queries = simulate_and_cluster(\n",
    "                    inst['prediction_hisotry'],\n",
    "                    inst['clusters'],\n",
    "                    **{k:v for k,v in alg.items() if k not in excluded}\n",
    "                )\n",
    "                new_clusters = {\n",
    "                    cid: {\n",
    "                        \"representative\": None if cid == \"EE\" else (\n",
    "                            {\"text\": data['responses'][0]['text'], \"log_prob\": data['responses'][0]['log_prob']}\n",
    "                            if data['responses'] else {\"text\": \"\", \"log_prob\": 0}\n",
    "                        ),\n",
    "                        \"probability\": data['probability'],\n",
    "                        \"num_members\": 0 if cid == \"EE\" else len(data['responses'])\n",
    "                    }\n",
    "                    for cid, data in clusters.items()\n",
    "                }\n",
    "                calib_simulated.append({\n",
    "                    \"question\": inst.get(\"question\", \"\"),\n",
    "                    \"true_answer\": inst.get(\"true_answer\", \"\"),\n",
    "                    \"clusters\": new_clusters,\n",
    "                    \"semantic_probs\": sem_probs\n",
    "                })\n",
    "\n",
    "            with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as tmp_calib:\n",
    "                json.dump(calib_simulated, tmp_calib, indent=2)\n",
    "                tmp_calib_path = tmp_calib.name\n",
    "\n",
    "            em_acc, best_tau, _ = alg['calibration_fn'](tmp_calib_path, alpha=alpha)\n",
    "            metrics_accum[name]['em_calib_accuracy'].append(em_acc)\n",
    "            threshold = best_tau\n",
    "\n",
    "            # --- Validation Phase ---\n",
    "            val_simulated = []\n",
    "            for inst in val_set:\n",
    "                resp, clusters, sem_probs, history, num_queries = simulate_and_cluster(\n",
    "                    inst['prediction_hisotry'],\n",
    "                    inst['clusters'],\n",
    "                    **{k:v for k,v in alg.items() if k not in excluded}\n",
    "                )\n",
    "\n",
    "                new_clusters = {\n",
    "                    cid: {\n",
    "                        \"representative\": None if cid == \"EE\" else (\n",
    "                            {\"text\": data['responses'][0]['text'], \"log_prob\": data['responses'][0]['log_prob']}\n",
    "                            if data['responses'] else {\"text\": \"\", \"log_prob\": 0}\n",
    "                        ),\n",
    "                        \"probability\": data['probability'],\n",
    "                        \"num_members\": 0 if cid == \"EE\" else len(data['responses'])\n",
    "                    }\n",
    "                    for cid, data in clusters.items()\n",
    "                }\n",
    "                val_simulated.append({\n",
    "                    \"question\": inst.get(\"question\", \"\"),\n",
    "                    \"true_answer\": inst.get(\"true_answer\", \"\"),\n",
    "                    \"clusters\": new_clusters,\n",
    "                    \"semantic_probs\": sem_probs\n",
    "                })\n",
    "\n",
    "            with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as tmp_val_in:\n",
    "                json.dump(val_simulated, tmp_val_in, indent=2)\n",
    "                tmp_val_in_path = tmp_val_in.name\n",
    "\n",
    "            with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as tmp_val_out:\n",
    "                tmp_val_out_path = tmp_val_out.name\n",
    "\n",
    "            alg['prediction_fn'](tmp_val_in_path, threshold, tmp_val_out_path)\n",
    "            coverage, avg_size, ee_frac, acc = compute_coverage_and_set_stats(tmp_val_out_path)\n",
    "\n",
    "\n",
    "            # Accumulate\n",
    "            metrics_accum[name]['coverage'].append(coverage)\n",
    "            metrics_accum[name]['ee_frac'].append(ee_frac)\n",
    "            metrics_accum[name]['avg_size'].append(avg_size)\n",
    "            metrics_accum[name]['accuracy'].append(acc)\n",
    "\n",
    "    # Compute mean and std for each metric\n",
    "    metrics_summary = {\n",
    "        name: {\n",
    "            metric: {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values)\n",
    "            }\n",
    "            for metric, values in metric_dict.items()\n",
    "        }\n",
    "        for name, metric_dict in metrics_accum.items()\n",
    "    }\n",
    "\n",
    "    return metrics_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235218dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\n",
    "    {\n",
    "        \"name\": \"bootstrap_derivative, Optimal Calibration\",\n",
    "        \"sampling_criteria\": \"derivative\",\n",
    "        \"derivative_type\": \"bootstrap\",\n",
    "        \"lambda_threshold\":  0.0015,\n",
    "        \"initial_derivative\": 3,\n",
    "        \"min_queries\": 10,\n",
    "        'calibration_fn': compute_accuracy_and_calibration_threshold_optimal,\n",
    "        'prediction_fn': generate_prediction_sets_optimal\n",
    "        # 'h' will default to 20 bootstrap replicates\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f6141",
   "metadata": {},
   "source": [
    "### Plotting - runnning the final simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d77b93",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/Server2019Test/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Cell 8: Plot metrics vs. 1 - α for all algorithms\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# Define the list of significance levels (α) to evaluate\n",
    "alphas = [0.2, 0.19, 0.18, 0.17, 0.16, 0.15, 0.14, 0.13, 0.12, 0.11, 0.1, 0.09, 0.08, 0.07, 0.06, 0.05]\n",
    "# Prepare a structure to hold metrics for each algorithm across alphas\n",
    "metrics_by_alpha = {\n",
    "    alg['name']: {\n",
    "        'coverage': {'mean': [], 'std': []},\n",
    "        'ee_frac': {'mean': [], 'std': []},\n",
    "        'avg_size': {'mean': [], 'std': []},\n",
    "        'accuracy': {'mean': [], 'std': []}\n",
    "    }\n",
    "    for alg in algorithms\n",
    "}\n",
    "\n",
    "# Loop over each α, run evaluation, and collect metrics\n",
    "for alpha in alphas:\n",
    "    print(f\"Evaluating at alpha = {alpha}\")\n",
    "    metrics = evaluate_algorithms_optimal(\n",
    "        dataset,    # your loaded full_results list\n",
    "        algorithms,\n",
    "        alpha=alpha,\n",
    "        n_splits=10\n",
    "    )\n",
    "    print(\"metrics\", metrics)\n",
    "    for alg_name, vals in metrics.items():\n",
    "        for metric_key in ['coverage', 'ee_frac', 'avg_size', 'accuracy']:\n",
    "            metrics_by_alpha[alg_name][metric_key]['mean'].append(vals[metric_key]['mean'])\n",
    "            metrics_by_alpha[alg_name][metric_key]['std'].append(vals[metric_key]['std'])\n",
    "\n",
    "#save in Drive so later we can plot\n",
    "\n",
    "# 1) Make sure your Drive path exists\n",
    "out_dir = \"/content/drive/MyDrive/CPQ_results\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "json_path = os.path.join(out_dir, \"triviaqa_appendix_GTgeoshapes_Ours_0.05.pkl\")\n",
    "payload = {\n",
    "    'alphas': alphas,                    # e.g. [0.1, 0.2, 0.3, …]\n",
    "    'metrics_by_alpha': metrics_by_alpha # your existing dict\n",
    "}\n",
    "\n",
    "with open(f\"{out_dir}/appendix_budegts_gsm8k_avg20_lambda0.0015.pkl\", \"wb\") as f:\n",
    "    pickle.dump(payload, f)\n",
    "print(\"Saved metrics_by_alpha (JSON) to\", json_path)\n",
    "'''\n",
    "for alpha in alphas:\n",
    "    print(f\"Evaluating at alpha = {alpha}\")\n",
    "    metrics = evaluate_algorithms_optimal(\n",
    "        dataset,    # your loaded full_results list\n",
    "        algorithms,\n",
    "        alpha=alpha,\n",
    "        n_splits=10\n",
    "    )\n",
    "    for alg_name, vals in metrics.items():\n",
    "        metrics_by_alpha[alg_name]['coverage'].append(vals['coverage'])\n",
    "        metrics_by_alpha[alg_name]['ee_frac'].append(vals['ee_frac'])\n",
    "        metrics_by_alpha[alg_name]['avg_size'].append(vals['avg_size'])\n",
    "        metrics_by_alpha[alg_name]['accuracy'].append(vals['accuracy'])'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
